{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchnlp.modules.transformer.sublayers import MultiHeadAttention, PositionwiseFeedForward\n",
    "from torchnlp.modules.normalization import LayerNorm\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents one Encoder layer of the Transformer Encoder\n",
    "    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            hidden_size: Hidden size\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            num_heads: Number of attention heads\n",
    "            bias_mask: Masking tensor to prevent connections to future elements\n",
    "            layer_dropout: Dropout for this layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n",
    "                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "        \n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding = 'both', \n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha = LayerNorm(hidden_size)\n",
    "        self.layer_norm_ffn = LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_mha(x)\n",
    "        input_ = x_norm\n",
    "        # Multi-head attention\n",
    "        y, attn, bias_mask = self.multi_head_attention(x_norm, x_norm, x_norm)\n",
    "        \n",
    "        # Dropout and residual\n",
    "        x = self.dropout(x + y)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        \n",
    "        # Positionwise Feedforward\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        \n",
    "        # Dropout and residual\n",
    "        y = self.dropout(x + y)\n",
    "        \n",
    "        return y, input_, attn, bias_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "atten_hook = []\n",
    "def hook(model, in_f, out_f):\n",
    "    print(\"hooking\")\n",
    "    model_names.append(model.__class__)\n",
    "    atten_hook.append((in_f, out_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = EncoderLayer(30, 30,30, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fab31bc98d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.multi_head_attention.register_forward_hook(hook=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/zijiao/research/atal/torchnlp/modules/transformer/sublayers.py\u001b[0m(106)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    105 \u001b[0;31m        \u001b[0;31m# Combine with values to get context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 106 \u001b[0;31m        \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    107 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooking\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d14f3d9e63fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2c7cfbf7a89a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Multi-head attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Dropout and residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 30)\n",
    "output=tt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.EncoderLayer"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False\n",
      "multi_head_attention True\n",
      "multi_head_attention.query_linear False\n",
      "multi_head_attention.key_linear False\n",
      "multi_head_attention.value_linear False\n",
      "multi_head_attention.output_linear False\n",
      "multi_head_attention.dropout False\n",
      "positionwise_feed_forward False\n",
      "positionwise_feed_forward.layers False\n",
      "positionwise_feed_forward.layers.0 False\n",
      "positionwise_feed_forward.layers.0.pad False\n",
      "positionwise_feed_forward.layers.0.conv False\n",
      "positionwise_feed_forward.layers.1 False\n",
      "positionwise_feed_forward.layers.1.pad False\n",
      "positionwise_feed_forward.layers.1.conv False\n",
      "positionwise_feed_forward.relu False\n",
      "positionwise_feed_forward.dropout False\n",
      "dropout False\n",
      "layer_norm_mha False\n",
      "layer_norm_ffn False\n"
     ]
    }
   ],
   "source": [
    "for name, m, in tt.named_modules():\n",
    "    (name, isinstance(m, MultiHeadAttention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zijiao/research/atal'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.data.conll import conll2000_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = conll2000_dataset(50)\n",
    "\n",
    "type(con)\n",
    "\n",
    "iters = con['iters']\n",
    "\n",
    "train_iter = iters[0]\n",
    "\n",
    "type(train_iter)\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "train_data = list(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  686,    7,  ...,    1,    1,    1],\n",
       "        [   2,   11,  148,  ...,    1,    1,    1],\n",
       "        [   2, 1090,   14,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 2894,  224,  ...,    1,    1,    1],\n",
       "        [   2,    5,  183,  ...,    1,    1,    1],\n",
       "        [   2,    5, 1298,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].inputs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 34,  7,  ...,  1,  1,  1],\n",
       "         [ 2,  7, 19,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "        [[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 36,  9,  ...,  1,  1,  1],\n",
       "         [ 2, 52,  6,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "        [[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 37, 11,  ...,  1,  1,  1],\n",
       "         [ 2, 28, 10,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 36, 18,  ...,  1,  1,  1],\n",
       "         [ 2, 34,  7,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "        [[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 30, 13,  ...,  1,  1,  1],\n",
       "         [ 2, 18, 12,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "        [[ 2,  2,  3,  ...,  1,  1,  1],\n",
       "         [ 2, 30, 13,  ...,  1,  1,  1],\n",
       "         [ 2, 52,  4,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1]]], device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].inputs_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8,  ..., 1, 1, 1],\n",
       "        [2, 6, 5,  ..., 1, 1, 1],\n",
       "        [2, 5, 5,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [2, 5, 4,  ..., 1, 1, 1],\n",
       "        [2, 5, 4,  ..., 1, 1, 1],\n",
       "        [2, 5, 4,  ..., 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where is tags\n",
    "train_data[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "con2 = conll2000_dataset(50)\n",
    "\n",
    "iters_2 = con2['iters']\n",
    "\n",
    "train_iter_2 = iters_2[0]\n",
    "\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "train_data_2 = list(train_iter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  686,    7,  ...,    1,    1,    1],\n",
       "        [   2,   11,  148,  ...,    1,    1,    1],\n",
       "        [   2, 1090,   14,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 2894,  224,  ...,    1,    1,    1],\n",
       "        [   2,    5,  183,  ...,    1,    1,    1],\n",
       "        [   2,    5, 1298,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].inputs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  686,    7,  ...,    1,    1,    1],\n",
       "        [   2,   11,  148,  ...,    1,    1,    1],\n",
       "        [   2, 1090,   14,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 2894,  224,  ...,    1,    1,    1],\n",
       "        [   2,    5,  183,  ...,    1,    1,    1],\n",
       "        [   2,    5, 1298,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2[0].inputs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,   10,    9,  ...,    1,    1,    1],\n",
       "         [   2,  477, 3164,  ...,    1,    1,    1],\n",
       "         [   2,    5, 2165,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   2,   36,   62,  ...,    1,    1,    1],\n",
       "         [   2,   35, 4343,  ...,    1,    1,    1],\n",
       "         [   2,  449,  348,  ...,    1,    1,    1]], device='cuda:0'),\n",
       " tensor([[   2,   10,    9,  ...,    1,    1,    1],\n",
       "         [   2,  477, 3164,  ...,    1,    1,    1],\n",
       "         [   2,    5, 2165,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   2,   36,   62,  ...,    1,    1,    1],\n",
       "         [   2,   35, 4343,  ...,    1,    1,    1],\n",
       "         [   2,  449,  348,  ...,    1,    1,    1]], device='cuda:0'))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].inputs_word, train_data_2[1].inputs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.data.conll import conll2000_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "con2 = conll2000_dataset(50)\n",
    "\n",
    "iters_2 = con2['iters']\n",
    "\n",
    "train_iter_2 = iters_2[0]\n",
    "\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "train_data_2 = list(train_iter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  686,    7,  ...,    1,    1,    1],\n",
       "        [   2,   11,  148,  ...,    1,    1,    1],\n",
       "        [   2, 1090,   14,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 2894,  224,  ...,    1,    1,    1],\n",
       "        [   2,    5,  183,  ...,    1,    1,    1],\n",
       "        [   2,    5, 1298,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2[0].inputs_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_2.shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_2.init_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2_2 = list(train_iter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  686,    7,  ...,    1,    1,    1],\n",
       "        [   2,   11,  148,  ...,    1,    1,    1],\n",
       "        [   2, 1090,   14,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 2894,  224,  ...,    1,    1,    1],\n",
       "        [   2,    5,  183,  ...,    1,    1,    1],\n",
       "        [   2,    5, 1298,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2_2[0].inputs_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] finished the data shuffle problem\n",
    "- [ ] make train, shufffle, make evaluate not shuffle\n",
    "- [ ] put save atttention in evalaution part\n",
    "- [ ] test load evaluation\n",
    "- [ ] finish the adversarial loss and start training\n",
    "REST\n",
    "---\n",
    "- [ ] use random pre attention as baseline, to compare the divergence\n",
    "- [ ] use above to test guiding classifiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('allennlp': conda)",
   "language": "python",
   "name": "python37664bitallennlpconda6b161ae5cf3e43a6ad2666aa947c4480"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
