# Transformer-Attention
The implementation is based on a NLP library, [kolloldas](https://github.com/kolloldas/torchnlp)
This project tries to investigate the exclusivity of attention mechanism (self-attention, multi-head) in Transformer model. For
details, please see the pdf file. 

**It is a rather preliminary work**
